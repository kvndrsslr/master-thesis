\chapter{Background}
\label{ch:sota}

\section{Genetic Algorithms}
\label{sec:ga}
\Iac{GA} is a non-deterministic population-based global search metaheuristic that reformulates optimization problems in a vocabulary that makes heavy usage of metaphors on biological evolution, thus also being called a nature-inspired metaheuristic.

\aclp{GA} typically represent solutions to a problem as bit vectors which correspond to the \emph{genotype} in biological evolution.
These genotypes encode a set of parameters which are passed to a so called \emph{fitness function} which promotes a survival of the fittest and is thus the target of minimization or maximization in the underlying optimization problem.

A fixed size population of genotypes, in this context also called individuals is initially generated at random using a \ac{PRNG}.
This population of genotypes, parameterized by the population size ($\mu$), is then iteratively evolved where the iterations are typically referred to as \emph{generations}.
In each generation, a subset of the population is selected to join a so called recombination pool, which is parameterized per the \emph{recombination pool size} ($\lambda$).
The selection of individuals is carried out using a so called selection operator.
An important property of a selection operator is the \emph{selective pressure}, which is informally described as the emphasis of selection on the best individual, where a high (small) selective pressure means a strong (weak) emphasis.
We will discuss two of the most prominent selection operators further down in this text.

The individuals selected for recombination are then passed in pairs to a crossover operator, which generates a pair of childs by swapping the bit vectors representing the individuals at a given crossover point\sidenote{Note that the literature contains a vast number of variations on the crossover operator such as multi-point crossover\cite{spears:1990a} and uniform crossover \cite{syswerda:1989a}, however they are of no particular interest to this work. There has also been a debate over the usefulness of crossover operators which resulted in  several extensive theoretical analyses with minimalistic toy problems with the result that they ,,can provably be useful''\cite{doerr:tcs2012a,spears:1991a}.}.
The childs are then inserted into the next generations population together with the survivors, which are selected from the current population also using a selection operator.

A mutation operator is then applied to each individual in the next generation with a certain \emph{mutation probability} ($\sigma$).
The classic mutation operator mutates each selected individual by changing the bit value of a given bit in its bit vector representation with a probability denoted by the \emph{mutation rate} ($\rho$), i.e. a mutation rate of $\rho=\frac{1}{2}$ means that on average, half of the bits are changed to their opposing state.

These steps of selection, crossover and mutation are repeated until a certain termination criterion holds, which could include a maximum number of generations, a target fitness value or a test of population convergence.

Note that in the sense as defined above, \aclp{GA} were reportedly\cite{mitchell:1998a} first conceived by John Holland in 1960 as an extension of general $(\mu +\lambda)$ Evolutionary Algorithms, the novelty being the introduction of crossover and selection operators whereas previous iterations of Evolutionary Algorithms only made use of mutation operators.

\paragraph{Selection Operators}
While the literature contains a great number of selection operators, for sake of brevity we will only introduce the two selection operators used in the remainder of this text, namely the Elitist Selection\cite{mitchell:1998a} and the Tournament Selection\cite{miller:cs1995a}.


The Elitist Selection is a simple selection procedure which includes the $N$ best performing individuals in the next generation.
It is typically used in conjunction with another selection operator, as it would just lead to a stochastic hill climbing algorithm when used as only selection operator and there are much more efficient hill climbing algorithms, such as \cite{vaughan:ijc2005a} for discrete or \cite{altman:cm1970a} for continuous optimization problems.

The Tournament Selection is one of the most established selection operators, as it has been proven able to adjust the selective pressure independently from the population size and fitness function scaling\cite{goldberg:1990a}.
It is parameterized by the tournament size $k$ and the selection probability $p$.
Initially, $k$ individuals from the given population are randomly selected to enter a tournament.
Then, the $i$th best individual in the tournament w.r.t. the fitness function is selected to win the tournament with the probability $p(1-p)^{i-1}$.
This means that setting the probablity to $p=1$ will result in a deterministic behaviour while setting $k=1$ results in random selection.

\newpage
\subsection{Genetic Programming}
\label{ssec:gp}

\acf{GP} is a subfield of \aclp{GA} which originated from the application of a \ac{GA} in order to evolve computer programs in 1988 by John Koza\cite{koza:1990a}.
While traditional \acp{GA} are commonly used for numerical optimization and search problems, \ac{GP} can be used for symbolic regression and classification.

In \acl{GP}, programs are usually encoded as trees of operations and terminals, but other encodings have also been proposed.
For example, Graff et al.\cite{graff:2016a} use \acp{DAG} to encode python programs, and Kvasnièka and Pospíchal\cite{kvasnieka:1998a} introduced a condensed encoding of \acp{DAG}, dubbed ,,Column Tables'', which we will adapt and use in this work (see \autoref{ssec:enrichmentgraph}).



\subsection{Multi Expresssion Programming}
\label{ssec:mep}

\acf{MEP}, originally introduced by Ferreira\cite{ferreira:cs2001a} as ,,Gene Expression Programming'' denotes a special kind of \acl{GP} where the genotype of an individual represents multiple solutions or subsolutions that has gained traction in recent years.
Only the best amongst these multiple solutions is then used as the effective solution, also called the \emph{phenotype} of the individual.
This technique is used in problems where it does incur no additional cost to keep track of the fitness of subsolutions, such as is generally the case with the evaluation of a program represented as a tree: it can be easily seen that when evaluating such a tree-shaped program, one has to visit each node anyway in order to compute its result.

It can be used to evolve programs with the same complexity as traditional \ac{GP} but much more efficient and excels in multiple-output problems.
To this end it has been successfully applied to the TSP\cite{oltean:c2015a}, data prediction\cite{zhang:i2013a}, 
software effort estimation\cite{akram:c2018a}, on-the-fly hyperparameter optimization for Evolutionary Algorithms\cite{oltean:2003a} and digital curcuit design\cite{oltean:2004b}.

\newpage
\subsection{Semantic Genetic Operators}
\label{ssec:sgo}

Semantic Genetic Operators are a special kinds of crossover and mutation operators which take into account the semantics of a genotype rather than just discerning it as a bit vector without any further interpretation.
They are therefore aware of the specific genotype encoding and manipulate the individuals w.r.t. the solution space rather than the encoding space.

Their recent successful application to a number of problems\cite{chen:2017a,forstenlechner:2017a,peng:ia2019a,suarez:rcs2015a} indicates that taking semantics into account is a promising novel direction for \ac{GP}.

\begin{table}[b]
%  \rowcolors{2}{gray!25}{white}
  \caption{The Five Stars of Linked Open Data}
  \label{tab:fivestar}
  \smaller
  \begin{tabularx}{\textwidth}{p{.15\textwidth}X>{\raggedleft\arraybackslash}p{.25\textwidth}}
  \toprule
%  \rowcolor{Maroon}
  Stars & Requirements & Example \\
  \midrule
    \score{1}{5} & Published with an Open License & PDF, PNG, TXT \\
    \score{2}{5} & Machine-Readable Structured Data & XLS, ODF \\
    \score{3}{5} & Non-proprietary File Format  &  XML, CSV, JSON\\
    \score{4}{5} & Use \acp{URI} to identify things &  RDF \\
    \score{5}{5} & Linked to other data & RDF \\
  \bottomrule
  \end{tabularx}
\end{table}

\newpage
\section{Linked Data}
\label{sec:ld}

Linked Data\cite{berners-lee:2006a, bizer:2008a, bizer:ijsis2009a} is a term coined by Tim Berners-Lee\cite{berners-lee:2006a} in 2006.
It refers to data available on the Web that is (1) structured, (2) uses \acp{URI} as identifiers and (3) is interlinked with other data on the Web.
Linked Data is enabled by the use of standard Web technologies such as the \ac{HTTP}, the \ac{RDF} and the \ac{SPARQL}.
It is closely related to the term Semantic Web\cite{berners-lee:sa2001a, shadbolt:iis2006a}, which describes a Web of Data which, in opposition to the human-readable Web of Documents is specifically engineered to be machine-readable.
The Semantic Web promotes a high interconnectedness of datasets by interlinking them with eath other, thereby enabling the ability to consume data from multiple datasets at once using semantic queries, e.g. using the \ac{SPARQL} protocol.\\

When considering Linked Data published under open licenses, we speak of \acf{LOD}.
A quality scheme for \ac{LOD}, namely the five stars of \ac{LOD}, was given by Tim Berners-Lee in 2010\cite{berners-lee:2006a} and is summarized in \autoref{tab:fivestar}.

\todo{Further Discussion of Linked Data?}

\subsection{The Resource Description Framework}
\label{ssec:rdf}

The \acf{RDF} is a ,,standard model for data interchange on the Web''\cite{group:2014a}.
\ac{RDF} models data in an abstract syntax which is defined in terms of an abstract graph-based data model.
The nodes in this graph-based model belong to one of three atomic base types: \emph{\acs{IRI} resources}, \emph{literals} and \emph{blank nodes}.
\acs{IRI} resources and blank nodes denote a particular subject in the world, while literals only contain values which are subject to further interpretation.
As blank nodes lack the notion of a global identifier, they serve as existential variables in the data model.

An \ac{RDF} graph is a collection of \emph{statements}, also called \emph{triples}, which basically correspond to simple sentences in natural language.
Each statement therefore consists of a subject, a predicate and an object.
Every subject is either a \acs{IRI} resource or a blank node, all predicates are \acs{IRI} resources and objects can be either of the three basic types.

In the following, we will give a formal specification of an \ac{RDF} dataset on which we will rely in the remainder of this work.

\newpage
\acused{IRI}
\paragraph{RDF Dataset:} Let $\mathcal{R}$ be the set of all \ac{RDF} \ac{IRI} resources, $\mathcal{B}$ be the set of all blank nodes and $\mathcal{L}$ be the set of all literals.

We call a set $D\coloneq \{(s,p,o) \in (\mathcal{R}\cup\mathcal{B})\times\mathcal{R}\times(\mathcal{R}\cup\mathcal{B}\cup\mathcal{L})\} $ of triples an \textbf{\ac{RDF} dataset}.\\

Note that for sake of simplicity, we will not distinguish between an \ac{RDF} dataset and its abstract graph and therefore we from now on resort to referring to both as \ac{RDF} datasets.\\

\subsection{Vocabularies, Ontologies and Shapes}
\label{ssec:onto}
While \ac{RDF} specifies a basic set of vocabulary, it is not very expressive on its own and offers no mechanism to define vocabularies or data schemes.

In order to make statements about the nature of the data that is expressed, other standards which extend on the \ac{RDF} semantics have to be used.

This is a concious decision as it allows to use multiple semantics, \ie~\acp{DL} of different expressiveness to be used in conjunction with the \ac{RDF} data model.

The two main standards that are used to that end are \ac{RDFS} and \ac{OWL}.\\

Contrary to the typical notion of ,,schema'', as for example in \ac{XSD}, \ac{RDFS} is not used to define a closed schema in order to force adherence to it.
Rather, it provides a set of standard predicates and resources that enable composing a simple ontology with mostly statements about class and predicate hierarchies, instances of classes and relationships between classes and predicates.
It thereby enables the use of efficient reasoners for basic inference as well as more standardized tools for applications to discover and reason about what the data is about and how it is potentially shaped.
The ,,potentially'' in the last sentence is owed to the fact that \ac{RDF} as well as \ac{RDFS} semantics follow the \emph{open-world assumption}, \ie~the absence of a fact is not enough to prove its negation holds true.\\

The \acl{OWL} is a standard which allows for much more expressiveness compared to \ac{RDFS}.
There are three subsets of \ac{OWL}: \ac{OWL} Lite $\subset$ \ac{OWL} DL $\subset$ \ac{OWL} Lite.
Their expressiveness increases from \ac{OWL} Lite to \ac{OWL} Full, which naturally comes at the cost of availability of efficient or even effective reasoning algorithms.\\

Another standard that can be used to accompany the \ac{RDF} data model is the \acf{SHACL}.
Contrary to the previously introduced standards, \ac{SHACL} semantics follow the \emph{Closed World Assumption} which on one hand decreases its effective usability alongside \ac{RDFS} and \ac{OWL} but on the other enables an important application that can not be realized with the latter two, namely that of schema validation.

\section{Linked Data Integration}
\label{sec:ldi}

Generally speaking, Linked Data Integration denotes a process of processing data whose goal it is to return one or many \ac{RDF} dataset(s) for a specific use case, \eg~publication to the \ac{LOD} Cloud or preparation for further usage by specific applications.

The input to Linked Data Integration can, but not necessarily must be Linked Data itself.
Typical processing steps in Linked Data Integration include, but are not limited to (1) lifting, (2) linking, (3) fusion, (4) enrichment.
Any one step could be left out, depending on the use case.

In the following, we will define each of these processing steps and give a narrow overview of the State of the Art.

\subsection{Lifting}
\label{ssec:lifting}
In the Linked Data community, lifting denotes the act of materializing \ac{RDF} datasets from sources of non-linked data.
These sources may be relational databases, proprietary file formats, measurements from sensor networks and any other structured or unstructured data that is not Linked Data.

The term is closely related to \ac{RDF} mapping, whereas in mapping the focus is often on ad-hoc transformation of an evolving non-linked data source in contrast to the materializing approach in lifting

After lifting, the data is expected to be at least four-star data according to the scheme presented in \autoref{tab:fivestar}.\\

In \cite{stuhmer:2009a}, the authors attempt to lift event data from the interaction with Web pages.
Bizer et al.\cite{bizer:2010a} introduced a framework for discovering mappings to relational data sources on the Web.
In \cite{neumaier:2017a}, \cite{ayed:2017a}, \cite{schandl:2010a} and \cite{fiorelli:2015a}, the authors proposed approaches for lifting metadata on data portals, entries in the Docker registry, entries in file systems and whole spreadsheets, respectively.
The approach in \cite{hasan:2011a} considers a framework for continuously lifting sensor data to Linked Data in order to exploit the semantic interconnectedness of the sensor measurings for situation awareness.

\subsection{Linking}
\label{ssec:linking}

The linking of \ac{RDF} datasets, sometimes also called instance matching, amounts to finding a mapping $M$ between two \ac{RDF} datasets, commonly called the source dataset $S$ and the target dataset $T$, such that the pairs $(s,t)\in M$ identified by the mapping abide to a given relation $r$.

This field is closely related to the instance matching and deduplication in databases.
The main challenge here is that due to the very large number of instances in the Web of Data, the quadratic run time complexity of a brute force search can not be accepted.

To this end, it is necessary to derive algorithms that discard large numbers of potential pairs, preferably without losing the completeness or correctness property, \ie~all pairs $(s,t)$ that abide by $r(s,t)$ should be in $M$ and no pair in $M$ should not abide by $r$.

A plethora of such approaches has been developed for textual data\cite{drng14,xiao:p2008a,xiao:tds2011a,feng:vj2012a,isele:2011a,karakasidis:2012a}, geospatial data\cite{radon_2017, du:2013a,shivaprabhu:2017a} as well as temporal data\cite{allenalgebra}.

Another challenge lies in the automatic configuration of so called link discovery tools.

A number of different approaches have been proposed.

The approaches in \cite{NGLY12,NGO+13b,isele:2013a} use \acl{GA} in an unsupervised, an active learning setting and both, respectively.
In \cite{NGLY13}, the authors use an altered grid search for unsupervised learning and finally \cite{sherif:2017a}, the approach outperforming all of the previously mentioned, uses refinement operators for unsupervised as well as supervised learning.

\subsection{Fusion}
\label{ssec:fusion}

The fusion of \ac{RDF} datasets is a task in which two interlinked datasets that contain information about the same set of instances are merged to produce a joint dataset.
To this end, an ontology mapping has to be generated in a first step.
Subsequently, the fusion is carried out following a set of so called fusion rules that are either predefined by the system, the user or dynamically learned.

In \cite{nikolov:2007a,nikolov:2008a,nikolov:2008b}, the authors use a fusion architecture called \emph{KnoFuss} which handles the end-to-end fusion of \ac{RDF} datasets.
The apporach in \cite{mendes:2012a} combines quality assessment and fusion of \ac{RDF} datasets.
A probabilistic framework is applied to the fusion problem in\cite{dong:2014a} and \cite{bryl:2014a} is an approach to automatic learning of fusion rules in the setting of cross-language fusion from the Wikipedia corpus.

\subsection{Enrichment}
\label{ssec:enrichment}

Enrichment is the least well-defined step in the Linked Data Integration lifecycle.
Informally, enrichment corresponds to applying a set of modifications to a given \ac{RDF} dataset such that it abides by certain use case-specific predefined criteria.
We will call this set of modifications, \ie~addition and deletion of triples from the source dataset, an enrichment workflow in the remainder of this text.
Examples of such modifications include the dereferencing of information from other datasets, the application of quality assurance methods to the dataset, the conformation of the used vocabulary or the filtering of its contents.

In practice this means, that \ac{RDF} dataset enrichment is a problem that greatly differs from use case to use case. 
Notable applications of enrichment in the literature include \cite{abel:2011a}, which considers the automatic extraction of interests from Twitter posts in order to enrich user profiles on the Social Web and \cite{choudhury:2009a}, which applies a ranking method to the Youtube tag space in order to enrich datasets on the \ac{LOD} Cloud with links to Youtube videos.

However, to our best knowledge, the only effort directed towards a general framework of \ac{RDF} dataset enrichment together with an automatic learning approach can be found in \cite{sherif:2015a}, where the authors propose a framework and learning algorithm called \ac{DEER}.
We will thus have a deeper analysis of \ac{DEER} in the following section, where we will try to identify potential shortcomings of the framework in order to underline the necessity for our contribution in this work, \ac{DEER2}.

\subsection{DEER}
\label{ssec:deer}
As we chose to build our approach based on the existing Open Source software \ac{DEER}, we are going to highlight specific areas in the original paper resp.\, software, where we see chances of substantial improvement.

\subsubsection*{On Restrictiveness of Sequential Chaining}
The \acl{DEER} models \ac{RDF} dataset enrichment workflows as ordered sequences of enrichment functions.
These enrichment functions take as input exactly one \ac{RDF} dataset and return an altered version of it by virtue of addition and deletion of triples.
It is argued that formally, any modification on a given dataset can be represented as a set of additions and deletions.

While this is certainly true, the application of two independent enrichment functions, that formally have no knowledge of each other, could be problematic as one function might delete triples which the other relies on in order to successfully apply the desired enrichment.\\

We therefore suggest replacing the simple sequential pipelining approach with a more sophisticated one based on arranging the enrichment functions in a \ac{DAG}.
In order to better distinguish our approach from the former, we will call atomic enrichment functions which allow for multiple inputs and multiple outputs \emph{enrichment operators}.

\subsubsection*{On Modularity and Attractiveness of the System}
The framework consists of five atomic enrichment functions that can be combined to form so called enrichment pipelines of arbitrary size.
The language used implies that these enrichment functions are only examples and potentially more could be implemented.
 
In order to improve the real-world suitability of our contribution \ac{DEER2}, we therefore will aim to attract developer interest by providing plugin facilities with low barriers to entry.

\subsubsection*{On Appropriateness of Fitness Function}
The refinement operator-based learning approach in \ac{DEER} is defined as the task of finding an appropriate enrichment pipeline given an input \ac{RDF} dataset $K$ and a target \ac{RDF} dataset $K'$.

As the enrichment functions typically require manual configuration, the problem is twofold: it is not only the correct sequence of enrichment functions that is subject to the learning process but also the correct configuration of the enrichment functions that are part of the pipeline.

This is done by a combination of iterative construction of the enrichment pipeline and self-configuration of enrichment functions at each iteration.

That is, in order to decide which enrichment function should be next added to the current pipeline, each available enrichment function is self configured using the supplied training data.
It is then applied to the result of the previous enrichment function and its result evaluated using a fitness function over the provided training data.

Note that one of the results of the evaluation of \ac{DEER} was that it is sufficient to provide only a single training example.
While it seems unusual for a learning method to rely on a single training example, it is a valid assertment in this specific case, as enrichment can be assumed to deal with more or less regularly shaped \ac{RDF} datasets.

The dataset produced by the intermediate pipeline in each iteration is denoted as $K_i$ with $i$ being the position of the enrichment function producing $K_i$ in the pipline.

The fitness function used is the $F_1$-score\cite{sasaki:ttm2007a} over the set of triples in $K_i$ and $K'$.\\

We argue that this choice of fitness function is not optimal in a modular setting where solutions can be expected to be constructed in an iterative manner.
We consider the following short example to illustrate the problem. \\

Let us assume without loss of generality that there exists one triple $t=(s,p,o)\in K$ for which we can intuitively identify at least one corresponding triple $t=(s',p',o')\in K'$.
Then a first enrichment function might only alter our input triples predicates, yielding the intermediate result $t_1=(s,p',o)\in K_1$. Now another enrichment function might alter the subject, yielding $t_1=(s',p',o)\in K_2$ before we apply the final enrichment function and get to the desired result $t_3=t'=(s',p',o')\in K_3$.
While intuitively there are clearly improvements in each intermediate result w.r.t. our target triple $t'$, a fitness function only measuring the correspondence between whole triples will not be able to detect these improvements.\\

We will therefore use an evenly weighted linear combination of the $F_1$-scores between all the subjects, all the predicates, all the objects and additionally all the whole triples in our input and target datasets.

\subsubsection*{On Information Masking}

A more subtle problem with the refinement operator-based approach in \ac{DEER} lies in the combination of deterministic learning and self configuration.
By assuming that the training example contains the necessary information for learning the pipeline, which is a sane assumption to make for regularly shaped input datasets, it is also implicitly assumed that it contains enough information for the \emph{self-configuration} of \emph{all} the enrichment functions.

As a trivial counterexample, consider an enrichment pipeline where we filter out all triples that contain a certain predicate $p_f$ using the enrichment function $e_3$.
This means that no triple $(s, p_f, o)$ will be present in the target training data.

Let us assume that some triple $(s', p', o')$ present in the target training data were constructed by another enrichment function $e_2$ from a triple $t_{masked}=(s, p_f, o)$ that was previously constructed by yet another enrichment function $e_1$.

Without the triple $t_{masked}$ in the training data, $e_1$ might not be able to determine its own configuration and therefore will never get picked.
In the end, our pipeline will never get learned using the deterministic bottom-up approach in \ac{DEER}.
We call the underlying problem information masking.\\

As a result of this analysis, we will use a non-deterministic approach based on \ac{GP} in \ac{DEER2}. We will complement this with an idea dubbed pre- and postcondition broadcasting which is developed in order to increase the chance of successful self configuration in the light of information masking.

\section{Additional Preliminary Definitions}

In this section we will introduce some additional definitions required to understand the remainder of this work which did not fit into one of the previous sections.

\subsection{Simplified Concise Bounded Descriptions}

Let $D$ be an \ac{RDF} dataset and $r\in D$ be an \ac{IRI} resource in that dataset.

We define the \textbf{neighbourhood} $\textit{neighbours}(r)$ of $r$ in $D$ as the set of triples that contain $r$ as their subject.
Formally, $\textit{neighbours}(r) \coloneq \{ (s, p, o) \in D \mid s \equiv r \}$.\\

Let $k\in\mathbb{N} > 0$.
The \ac{SCBD} $\textit{SCBD}(r, k)$ of a resource $r$ in $D$ is then recursively defined per \autoref{eq:cbd} where $\textit{SCBD}(r, 0) \coloneq \emptyset$ and $\textit{SCBD}(r, 1) \coloneq \textit{neighbours}(r)$.

\begin{equation}
  \textit{SCBD}(r, k) \coloneq \textit{SCBD}(r, k-1) \cup \left( \bigcup_{(s, p, o) \in \textit{SCBD}(r, k-1)} \textit{neighbours}(o) \right)
  \label{eq:cbd}
\end{equation}

\subsection{The \texorpdfstring{$F_\beta$}{F-beta}-Score}
\label{ssec:fscore}
The $F_\beta$-score is a measure of a tests accuracy, or more abstract, a measure of correspondence between two sets.
Let $A, B$ be two sets, where we call $A$ the evaluated set and $B$ the reference set.
We define the precision and the recall of $A$ w.r.t. $B$ per \autoref{eq:precision} and \autoref{eq:recall}, respectively.
The $F_\beta$-score is then defined as the weighted harmonic mean of recall and precision as per \autoref{eq:fscore}.

\begin{align}
\textit{precision}(A,B) & = |A\cap B|/|A| \label{eq:precision} \\
\textit{recall}(A,B) & = |A\cap B|/|B| \label{eq:recall} \\ 
 F_\beta(A,B) & =  (1 + \beta^2) \cdot \frac{\textit{precision}(A,B) \cdot \textit{recall}(A,B)}{(\beta^2 \cdot \textit{precision}(A,B)) + \textit{recall}(A,B)}  \label{eq:fscore}
\end{align}


\subsection{Directed Acyclic Graphs}

A directed graph is a pair $\mathbf{G}=(\mathbf{V},\mathbf{E})$, where $\mathbf{V}$ denotes a finite set, called vertices, and $\mathbf{E}\subseteq \mathbf{V} \times \mathbf{V}$ denotes a set of 2-tuples, called edges.
An entry $(u,v)\in \mathbf{E}$ represents a directed edge from $u$ to $v$.

A sequence of edges $((u_1,v_1),\dots,(u_n,v_n))\in \mathbf{E}^n$ for which $v_k=u_{k+1}$ $\forall 1\leq k \leq n-1$ holds, is called a walk of length $n$ from $u_1$ to $v_n$ in $\mathbf{G}$.\\

We call a directed graph $\mathbf{G}=(\mathbf{V},\mathbf{E})$ \acf{DAG} iff:
\begin{enumerate}
  \item $\forall (u,v)\in\mathbf{E} : u \neq v $ and
  \item there exists no walk from $v$ to itself
\end{enumerate}

Note that the second criterion is equivalent to the the demand that all walks in $\mathbf{G}$ are of finite length.


